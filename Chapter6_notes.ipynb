{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Linear Model Selection and Regularization\n",
    "\n",
    "## Ch6. Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model has distinct advantages in terms of intference and, on real-world problems, is often surprisingly competitive in relation to non-linear methods. This chapter discussing some ways in which the simple linear model can be improved, by replacing plain least squares fitting with some alternative fitting procedures.\n",
    "\n",
    "Why, you ask, might we want to use another fitting procedure instead of least squares? We will see that alternative fitting procedures can yield better *_prediction accuracy_* and *_model interpretability_*.\n",
    "\n",
    "* **_Prediction Accuracy_**: Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. If *_n >> p_*, that is, the number of observations is much larger than variables, then the least squares estimates tend to also have low variance, and hence will perform well on test observations. *_However_*, if *_n_* is not much larger than *_p_*, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. And if *_p > n_*, then there is no longer a unique least squares coefficient estimate: the variance is *_infinite_* so the method cannot be used at all. By **_constraining_** or **_shrinking_** the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.\n",
    "\n",
    "* **_Model Interpretability_**: It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such *_irrelevant_* variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. In this chapter, we see some approaches for automatically performing **_feature selection_** or **_variable selection_**—that is, for excluding irrelevant variables from a multiple regression model.\n",
    "\n",
    "This chapter outlines three important classes of methods.\n",
    "\n",
    "* **_Subset Selection_**: This approach involves identifying a subset of the *_p_* predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\n",
    "\n",
    "* **_Shrinkage_**: This approach involves fitting a model involving all *_p_* predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as **_regularization_**) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.\n",
    "\n",
    "* **_Dimension Reduction_**: This approach involves projecting the *_p_* predictors into a *_M_*-dimensional subspace, where *_M < p_*. This is achieved by computing *_M_* different **_linear combinations_**, or **_projections_**, of the variables. Then these *_M_* projections are used as predictors to fit a linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Subset Selection** considers $2^p$models. For exaple if *_p_*=10, then there are 1,000 possible models to consider and if *_p_*=20 there are over 1 million possibilities. Consequently, best subset selection becomes computationally infeasible for values of *_p_* greater than around 40, even with extremely fast modern computers.\n",
    "\n",
    "Stepwise selection provides more computation feasible approaches.\n",
    "\n",
    "**Forward Stepwise Selection** Considers $1+(p-k)$ models. Starts with a null model, containing no predictors. Then adds predictors incrementally that provide the greates additional improvement to the fit of the model. Best is defined as having smallest RSS or highest $R^2$. Can be used when $n<p$.\n",
    "\n",
    "**Backward Stepwise Selection** also considers $1+(p-k)$ models. Starts with he full least squares model containing all *_p_* predictors, and the iteratively removes the least useful predictor, one-at-a-time. Requires $n>p$.\n",
    "\n",
    "Though both forward and backward stepwise selection ten to do well in practice, neither are not guaranteed to yield the best model containing a subset of the *_p_* predictors. Basically, this is because stepwise contains the previous steps of selection, whereas the *_best_* model may not include the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the optimal model** can be done with 2 common approaches:\n",
    "\n",
    "1. We can **_indirectly_** estimate test error by making an *_adjustment_* to the training error to acount for the bias due to overfitting.\n",
    "    1. Techniques for adjusting the training error fo the model size include: $C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted $R^2$.\n",
    "2. We can *__directly__* estimate the test error, using either a validation set approach or a cross-validation approach.\n",
    "    1. This procedure has an advantage relative to AIC, BIC, $C_p$, and adjusted $R^2$, in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model.\n",
    "    \n",
    "**Summary: choosing the optimal model directly or indirectly:**\n",
    "In the past, performing cross-validation was computationally prohibitive for many problems with large $p$ and/or large $n$, and so AIC, BIC, $C_p$, and adjusted $R^2$ were more attractive approaches for choosing among a set of models. **However**, nowadays with fast computers, the computations required to perform cross-validation are hardly ever an issue. Thus, **_cross-validation is a very attractive approach for selecting from among a number of models under consideration_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage Methods\n",
    "\n",
    "Subset selection methods involves using a subset of the full predictors. As an alternative, we can fit a model containing all $p$ predictors using a technique that *_constrains_* or *_regularizes_* the coefficient estimates, or equivalently, that *_shrinks_* the coefficient estimates toward zero.\n",
    "\n",
    "It turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coeffients towards zero are **_ridge regression_** and the **_lasso_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Both Ridge Regression and Lasso require standardized predictors. (For each predictor, subtract the mean and then divide by the standard deviation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "Recall from Chapter 3 that the least squares fitting procedure estimates $\\beta_0, \\beta_1,...,\\beta_p$ using the values that minimize:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij} \\right)^2$$\n",
    "\n",
    "Ridge regession is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. The ridge regression coefficient estimates $\\hat{\\beta}^R$ are the values that minimize:\n",
    "\n",
    "$$\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "Where $\\lambda >= 0$ is a tuning parameter, to be determined separately. \n",
    "\n",
    "As with least squared, ridge regression seeks coeffient estimates that fit the data well, by making the RSS small. However, the second term, $\\lambda \\sum_{j=1}^p \\beta_j^2$, called a **_shrinkage penalty_** (L2 in ridge regression) is small when $\\beta_1,...,\\beta_p$ are close to zero, and so it has the effect of shrinking the estimates of $\\beta_j$ towards zero.\n",
    "\n",
    "When $\\lambda=0$, the penalty term has no effect, and ridge regression will product the least squares estimates. However, as $\\lambda\\rightarrow \\infty$, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\n",
    "\n",
    "There is a trade-off between the penalty term and RSS. Maybe a large $\\beta$ would give you a better residual sum of squares but then it will push the penalty term higher. This is why you might actually prefer smaller $\\beta$'s with worse residual sum of squares. From an optimization perspective, the penalty term  is equivalent to a constraint on the $\\beta$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Lasso\n",
    "\n",
    "Ridge regression will skrink the coefficients towards zero (as $\\lambda\\rightarrow \\infty$), but will not *_equal_* zero. Lasso, on the other hand can zero out coefficients. This can lead to sparse models and can be interpreted as feature selection.\n",
    "\n",
    "The lasso coefficients, $\\hat{\\beta^L}_\\lambda$, minimize the quantity:\n",
    "\n",
    "$$\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "The lasso uses an L1 penalty instaed of an L2 penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization adds a bit of bias via the L1 or L2 penaltly, but gets greater improvement in the variance.\n",
    "\n",
    "* **L1 penalty**\n",
    "    * $\\sum_{j=1}^p |\\beta_j|$\n",
    "    * the absolute value of the magnitude of the coefficients.\n",
    "    * Manhattan distance\n",
    "    * lasso - Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "\n",
    "* **L2 penalty**\n",
    "    * $\\sum_{j=1}^p (\\beta_j)^2$\n",
    "    * the square of the magnitude of the coefficients.\n",
    "    * Euclidian distance\n",
    "    * ridge regression\n",
    "    \n",
    "Depending on the value of $\\lambda$, the lasso can produce a model involving any number of variables. Wheras, ridge regression will always include all the variables in the model, although the magnitude of the coefficient estimates will depend on $\\lambda$.\n",
    "\n",
    "#### Selecting the tuning parameter $\\lambda$\n",
    "$\\lambda$ is selected through cross-validation. We choose a grid of $\\lambda$ values, and compute the cross-validation error for each value of $\\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all the available observations and the selected value of the tuning parameter.\n",
    "\n",
    "#### Comparing the Lasso and Ridge Regression\n",
    "* Lasso will perform better when a relatively small number of predictors have substantial coefficients, while the remaining predictors have very small coefficients. \n",
    "* Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size.\n",
    "\n",
    "However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Variable Selection Property of the Lasso\n",
    "\n",
    "The blue diamond and circle represent the lasso and ridge regression contraints respectively.\n",
    "\n",
    "The ellipsess that are centered around $\\hat{\\beta}$ represent regions of constant RSS. All points on a given ellipse share a common value of the RSS.\n",
    "\n",
    "if $\\lambda=0$, then the constrain regions will contain $\\hat{\\beta}$. In this case, the least squared estimate lie outside the diamond and the circle. So the least squares estimate are not the same as the lasso and ridge regression estimates. (i.e. $\\lambda>0$)\n",
    "\n",
    "The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contact the constrain region (blue diamond and circle.)\n",
    "\n",
    "* Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero.\n",
    "\n",
    "* However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson05/image_09/index.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction Methods\n",
    "\n",
    "Previous methods in this chapter controlled variance in two different ways, either using a subset of the original variables (best subset, forward/backward selection) or by shrinking their coefficients toward zero (ridge regression and lasso). All these methods are defined using the original predictors. There is also a class of approaches called **_dimension reduction_** methods. These, first *_transform_* the predictors, then fit a model using the transformed predictors.\n",
    "\n",
    "#### Principal Components Analysis (PCA)\n",
    "\n",
    "A popular approach for deriving a low-dimensional set of features from a large set of variables.\n",
    "\n",
    "Successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.\n",
    "\n",
    "The first principal component $Z_1$ is the direction of the data is that along which the observations vary the most.\n",
    "\n",
    "The second principal component $Z_2$ is a linear combination of the variables that is uncorrelated with $Z_1$, and has largest variance subject to this constraint. \n",
    "\n",
    "The zero correlation condition of $Z_1$ with $Z_2$ is equivalent to the condition that the direction must be *_perpendicular_* or *_orthogonal_*, to the first principal component direction.\n",
    "\n",
    "Generally recommended to standardize each predictor prior to generating the principal components. If you don't standardize, the high-variance variables will play an outsized effect on the principal components obtained.\n",
    "\n",
    "#### Principal Components Regression (PCR)\n",
    "\n",
    "PCR involves constructing the first $M$ principal components, $Z_1,...,Z_M$, and then using these components as the predictors in a linear regression model that is fit using least squares.\n",
    "\n",
    "Principal Components Regression will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response. Otherwise, ridge or lasso may be better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson05/PCA_plot_02/index.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Least Squares (PLS)\n",
    "\n",
    "A supervised alternative to PCR. The response $Y$ is used to help determine the principal component directions.\n",
    "\n",
    "Makes use of the response $Y$ in order to identify new features that not only approximate the old features well, but also are related to the response.\n",
    "\n",
    "PLS computes the first direction $Z_1$ by setting each loading equal to the coefficient from the simple linear regression of $Y$ onto $X_j$. PLS places the highest weight on the variables that are most strongly related to the response.\n",
    "\n",
    "As with PCR, the number of $M$ partial least squares used in PLS is a tuning parameter typically chosen by cross-validation.\n",
    "\n",
    "Generally, you standardize your predictors before performing PLS.\n",
    "\n",
    "The overall benefit of PLS relative to PCR is a wash.\n",
    "\n",
    "#### Problems in High Dimensions\n",
    "\n",
    "Data sets containing more features than observations $(n<p)$ are often referred to as *_high-dimensional_*.\n",
    "\n",
    "In the high-dimensional setting least squares will overfit the data (a perfect fit) and the residuals will be zero.\n",
    "\n",
    "The methods outlined in this chapter (stepwise selection, ridge regression, lasso, and PCA are useful for performing regression in the high-dimensional setting. These approaches avoid overfitting by using a less flexible approach than least squares.\n",
    "\n",
    "**_Curse of dimensionality_**: i.e. more data can be a double edged sword. Adding additional signal features (variables) does not necessarily increase the accuracy of a model. Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring.\n",
    "\n",
    "In the high-dimensional setting, the multicollinearity problem is extreme.\n",
    "\n",
    "Never use RSS, p_values, $R^2$ statistics, or other traditional measures of model fit on the training data as evidence of a good fit in the high-dimesnsional setting.\n",
    "\n",
    "As we have learned by now, **__always evalute model performance on an independent test set or cross-validation__**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Lab 1: Subset Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Hitters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (322, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlayerName</th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>...</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Andy Allanson</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-Alan Ashby</td>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>...</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Alvin Davis</td>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-Andre Dawson</td>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-Andres Galarraga</td>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PlayerName  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
       "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
       "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
       "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
       "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
       "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
       "\n",
       "   CHits    ...      CRuns  CRBI  CWalks  League Division PutOuts  Assists  \\\n",
       "0     66    ...         30    29      14       A        E     446       33   \n",
       "1    835    ...        321   414     375       N        W     632       43   \n",
       "2    457    ...        224   266     263       A        W     880       82   \n",
       "3   1575    ...        828   838     354       N        E     200       11   \n",
       "4    101    ...         48    46      33       N        E     805       40   \n",
       "\n",
       "   Errors  Salary  NewLeague  \n",
       "0      20     NaN          A  \n",
       "1      10   475.0          N  \n",
       "2      14   480.0          A  \n",
       "3       3   500.0          N  \n",
       "4       4    91.5          N  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(index=str, columns={\"Unnamed: 0\": \"PlayerName\"})\n",
    "print(\"Shape of dataframe: \" + str(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify missing observations**\n",
    "\n",
    "59 players missing 'Salary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayerName     0\n",
       "AtBat          0\n",
       "Hits           0\n",
       "HmRun          0\n",
       "Runs           0\n",
       "RBI            0\n",
       "Walks          0\n",
       "Years          0\n",
       "CAtBat         0\n",
       "CHits          0\n",
       "CHmRun         0\n",
       "CRuns          0\n",
       "CRBI           0\n",
       "CWalks         0\n",
       "League         0\n",
       "Division       0\n",
       "PutOuts        0\n",
       "Assists        0\n",
       "Errors         0\n",
       "Salary        59\n",
       "NewLeague      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at all variables for missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at only 'Salary' for missing data\n",
    "df['Salary'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop all the rows that have missing values in any variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (263, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(\"Shape of dataframe: \" + str(df.shape))\n",
    "df['Salary'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>403.642586</td>\n",
       "      <td>107.828897</td>\n",
       "      <td>11.619772</td>\n",
       "      <td>54.745247</td>\n",
       "      <td>51.486692</td>\n",
       "      <td>41.114068</td>\n",
       "      <td>7.311787</td>\n",
       "      <td>2657.543726</td>\n",
       "      <td>722.186312</td>\n",
       "      <td>69.239544</td>\n",
       "      <td>361.220532</td>\n",
       "      <td>330.418251</td>\n",
       "      <td>260.266160</td>\n",
       "      <td>290.711027</td>\n",
       "      <td>118.760456</td>\n",
       "      <td>8.593156</td>\n",
       "      <td>535.925882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>147.307209</td>\n",
       "      <td>45.125326</td>\n",
       "      <td>8.757108</td>\n",
       "      <td>25.539816</td>\n",
       "      <td>25.882714</td>\n",
       "      <td>21.718056</td>\n",
       "      <td>4.793616</td>\n",
       "      <td>2286.582929</td>\n",
       "      <td>648.199644</td>\n",
       "      <td>82.197581</td>\n",
       "      <td>331.198571</td>\n",
       "      <td>323.367668</td>\n",
       "      <td>264.055868</td>\n",
       "      <td>279.934575</td>\n",
       "      <td>145.080577</td>\n",
       "      <td>6.606574</td>\n",
       "      <td>451.118681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>282.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>842.500000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>413.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1931.000000</td>\n",
       "      <td>516.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>526.000000</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3890.500000</td>\n",
       "      <td>1054.000000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>497.500000</td>\n",
       "      <td>424.500000</td>\n",
       "      <td>328.500000</td>\n",
       "      <td>322.500000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>687.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>14053.000000</td>\n",
       "      <td>4256.000000</td>\n",
       "      <td>548.000000</td>\n",
       "      <td>2165.000000</td>\n",
       "      <td>1659.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1377.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2460.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AtBat        Hits       HmRun        Runs         RBI       Walks  \\\n",
       "count  263.000000  263.000000  263.000000  263.000000  263.000000  263.000000   \n",
       "mean   403.642586  107.828897   11.619772   54.745247   51.486692   41.114068   \n",
       "std    147.307209   45.125326    8.757108   25.539816   25.882714   21.718056   \n",
       "min     19.000000    1.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%    282.500000   71.500000    5.000000   33.500000   30.000000   23.000000   \n",
       "50%    413.000000  103.000000    9.000000   52.000000   47.000000   37.000000   \n",
       "75%    526.000000  141.500000   18.000000   73.000000   71.000000   57.000000   \n",
       "max    687.000000  238.000000   40.000000  130.000000  121.000000  105.000000   \n",
       "\n",
       "            Years        CAtBat        CHits      CHmRun        CRuns  \\\n",
       "count  263.000000    263.000000   263.000000  263.000000   263.000000   \n",
       "mean     7.311787   2657.543726   722.186312   69.239544   361.220532   \n",
       "std      4.793616   2286.582929   648.199644   82.197581   331.198571   \n",
       "min      1.000000     19.000000     4.000000    0.000000     2.000000   \n",
       "25%      4.000000    842.500000   212.000000   15.000000   105.500000   \n",
       "50%      6.000000   1931.000000   516.000000   40.000000   250.000000   \n",
       "75%     10.000000   3890.500000  1054.000000   92.500000   497.500000   \n",
       "max     24.000000  14053.000000  4256.000000  548.000000  2165.000000   \n",
       "\n",
       "              CRBI       CWalks      PutOuts     Assists      Errors  \\\n",
       "count   263.000000   263.000000   263.000000  263.000000  263.000000   \n",
       "mean    330.418251   260.266160   290.711027  118.760456    8.593156   \n",
       "std     323.367668   264.055868   279.934575  145.080577    6.606574   \n",
       "min       3.000000     1.000000     0.000000    0.000000    0.000000   \n",
       "25%      95.000000    71.000000   113.500000    8.000000    3.000000   \n",
       "50%     230.000000   174.000000   224.000000   45.000000    7.000000   \n",
       "75%     424.500000   328.500000   322.500000  192.000000   13.000000   \n",
       "max    1659.000000  1566.000000  1377.000000  492.000000   32.000000   \n",
       "\n",
       "            Salary  \n",
       "count   263.000000  \n",
       "mean    535.925882  \n",
       "std     451.118681  \n",
       "min      67.500000  \n",
       "25%     190.000000  \n",
       "50%     425.000000  \n",
       "75%     750.000000  \n",
       "max    2460.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
